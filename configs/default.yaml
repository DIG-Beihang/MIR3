# Hierarchical Configurations
token: "train"
device: "cuda:0"
seed: 0
training_threads: 32
evaluate: False
eval_type: "minq"
change_t: -1
eval_iter: 10
eval_epsilon: 1
eval_alpha: 0.1
save_replay: False

# adversarial
adversarial: "none"

# policies
victim_policy:
  agent_name: ddpg
  agent_args:
    hidden_dim: 64
    use_rnn: True
    share_params: True

  learner_name: maddpg
  learner_args:
    noise_scale: 0.1
    start_steps: 0
    gamma: 0.99
    use_adam: True
    actor_lr: 0.0005
    critic_lr: 0.0005
    optim_alpha: 0.99
    optim_eps: 0.00000001
    max_grad_norm: 10
    action_reg: 0
    target_update_hard: False
    target_update_interval: 0.01

  checkpoint_path: ""

adversary_policy:
  adv_agent_ids: [0]

  agent_name: ddpg
  agent_args:
    hidden_dim: 400
    use_rnn: False
    share_params: True

  learner_name: maddpg
  learner_args:
    noise_scale: 0.1
    start_steps: 0
    gamma: 0.99
    use_adam: True
    actor_lr: 0.0005
    critic_lr: 0.0005
    optim_alpha: 0.99
    optim_eps: 0.00000001
    max_grad_norm: 10
    action_reg: 0
    target_update_hard: False
    target_update_interval: 0.01

  checkpoint_path: ""


# collect and buffer
sample_timestep: False
parallel_num: 1
train_epochs: 1
batch_size: 32
num_batches: 1
buffer_size: 5000
total_timesteps: 5000000

eval_episodes: 1
save_interval: 100000
eval_interval: 10000
log_interval: 5000

# log keys
sum_keys: [return]
last_keys: [ep_length]
mean_keys: []
